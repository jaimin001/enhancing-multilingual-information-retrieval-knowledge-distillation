{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, LoggingHandler, models, evaluation, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers.datasets import ParallelSentencesDataset\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import sentence_transformers.util\n",
    "import csv\n",
    "import gzip\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_name = 'paraphrase-distilroberta-base-v2'   #Our monolingual teacher model, we want to convert to multiple languages\n",
    "student_model_name = 'xlm-roberta-base'       #Multilingual base model we use to imitate the teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128                #Student model max. lengths for inputs (number of word pieces)\n",
    "train_batch_size = 64               #Batch size for training\n",
    "inference_batch_size = 64           #Batch size at inference\n",
    "max_sentences_per_language = 500000 #Maximum number of  parallel sentences for training\n",
    "train_max_sentence_length = 250     #Maximum length (characters) for parallel training sentences\n",
    "\n",
    "num_epochs = 5                       #Train for x epochs\n",
    "num_warmup_steps = 10000             #Warumup steps\n",
    "\n",
    "num_evaluation_steps = 1000          #Evaluate performance after every xxxx steps\n",
    "dev_sentences = 1000                 #Number of parallel sentences to be used for development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the language codes you would like to extend the model to\n",
    "source_languages = set(['en'])                      # Our teacher model accepts English (en) sentences\n",
    "target_languages = set(['de'])    # We want to extend the model to these new languages. For language codes, see the header of the train file file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"output/make-multilingual-\"+\"-\".join(sorted(list(source_languages))+sorted(list(target_languages)))+\"-\"+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 01:54:47 - Load teacher model\n",
      "2023-04-28 01:54:47 - Load pretrained SentenceTransformer: paraphrase-distilroberta-base-v2\n",
      "2023-04-28 01:54:48 - Lock 140007271883104 acquired on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/.gitattributes.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84cc54bc2ac4340a012404332611d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)2b9e5/.gitattributes:   0%|          | 0.00/736 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 01:54:48 - Lock 140007271883104 released on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/.gitattributes.lock\n",
      "2023-04-28 01:54:49 - Lock 140007211149008 acquired on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/1_Pooling/config.json.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628c517cc0004965b9cefbc3a66e46a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 01:54:49 - Lock 140007211149008 released on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/1_Pooling/config.json.lock\n",
      "2023-04-28 01:54:49 - Lock 140007212492304 acquired on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/README.md.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4bdb724f0d4e21b2a62dfb91abcf4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)3c1ed2b9e5/README.md:   0%|          | 0.00/3.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 01:54:49 - Lock 140007212492304 released on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/README.md.lock\n",
      "2023-04-28 01:54:50 - Lock 140007212492304 acquired on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/config.json.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61907f9b2531422aa3ddf57f62ec9c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)1ed2b9e5/config.json:   0%|          | 0.00/686 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 01:54:50 - Lock 140007212492304 released on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/config.json.lock\n",
      "2023-04-28 01:54:50 - Lock 140007212492304 acquired on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/config_sentence_transformers.json.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b818bdef4a4d91805e0733730753c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 01:54:51 - Lock 140007212492304 released on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/config_sentence_transformers.json.lock\n",
      "2023-04-28 01:54:51 - Lock 139998581270224 acquired on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/merges.txt.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c17eec0999345ef82519cde24f70801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)c1ed2b9e5/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 01:54:52 - Lock 139998581270224 released on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/merges.txt.lock\n",
      "2023-04-28 01:54:52 - Lock 140007212492304 acquired on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/pytorch_model.bin.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768d0ac004ad4e3baea88b801b9161af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 01:56:27 - Lock 140007212492304 released on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/pytorch_model.bin.lock\n",
      "2023-04-28 01:56:28 - Lock 140007211149632 acquired on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/sentence_bert_config.json.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41ca2b7c3134043900fa0a0190487e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 01:56:28 - Lock 140007211149632 released on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/sentence_bert_config.json.lock\n",
      "2023-04-28 01:56:28 - Lock 140007212493168 acquired on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/special_tokens_map.json.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1a00b737614af1963f1d464a53d649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 01:56:29 - Lock 140007212493168 released on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/special_tokens_map.json.lock\n",
      "2023-04-28 01:56:29 - Lock 140007271883104 acquired on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/tokenizer.json.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf61403640dd4a2fa97d17781846a7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)2b9e5/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 01:56:29 - Lock 140007271883104 released on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/tokenizer.json.lock\n",
      "2023-04-28 01:56:30 - Lock 140007212492304 acquired on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/tokenizer_config.json.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a23c6c8a3644f419ba676b06dcfd0ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 01:56:30 - Lock 140007212492304 released on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/tokenizer_config.json.lock\n",
      "2023-04-28 01:56:30 - Lock 140007212492304 acquired on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/vocab.json.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d75768b50346829f279284e684bb8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)c1ed2b9e5/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 01:56:31 - Lock 140007212492304 released on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/vocab.json.lock\n",
      "2023-04-28 01:56:31 - Lock 140007212493168 acquired on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/modules.json.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c72461a798b405583c3bc778206984b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ed2b9e5/modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 01:56:32 - Lock 140007212493168 released on /home/anvisa/.cache/torch/sentence_transformers/sentence-transformers_paraphrase-distilroberta-base-v2/modules.json.lock\n",
      "2023-04-28 01:56:32 - Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "######## Start the extension of the teacher model to multiple languages ########\n",
    "logger.info(\"Load teacher model\")\n",
    "teacher_model = SentenceTransformer(teacher_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 01:56:33 - Create student model from scratch\n",
      "2023-04-28 01:56:33 - Lock 140007212897664 acquired on /home/anvisa/.cache/huggingface/hub/models--xlm-roberta-base/blobs/1960141250d189366dfb76630ba794a9c104ec07.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81e5c861fc044a0845639c8fc4f1594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 01:56:33 - Lock 140007212897664 released on /home/anvisa/.cache/huggingface/hub/models--xlm-roberta-base/blobs/1960141250d189366dfb76630ba794a9c104ec07.lock\n",
      "2023-04-28 01:56:33 - Lock 139998573166448 acquired on /home/anvisa/.cache/huggingface/hub/models--xlm-roberta-base/blobs/9d83baaafea92d36de26002c8135a427d55ee6fdc4faaa6e400be4c47724a07e.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8da1bbfe404ed587eceb4f19841a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 02:02:51 - Lock 139998573166448 released on /home/anvisa/.cache/huggingface/hub/models--xlm-roberta-base/blobs/9d83baaafea92d36de26002c8135a427d55ee6fdc4faaa6e400be4c47724a07e.lock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 02:02:54 - Lock 139998572426384 acquired on /home/anvisa/.cache/huggingface/hub/models--xlm-roberta-base/blobs/db9af13bf09fd3028ca32be90d3fb66d5e470399.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e181bf5f55485f94e457f428078af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 02:02:57 - Lock 139998572426384 released on /home/anvisa/.cache/huggingface/hub/models--xlm-roberta-base/blobs/db9af13bf09fd3028ca32be90d3fb66d5e470399.lock\n",
      "2023-04-28 02:02:58 - Lock 139998573109408 acquired on /home/anvisa/.cache/huggingface/hub/models--xlm-roberta-base/blobs/463f3414782c1c9405828c9b31bfa36dda1f45c5.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051d41012d8d4a768d7d214184f6d4b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 02:03:00 - Lock 139998573109408 released on /home/anvisa/.cache/huggingface/hub/models--xlm-roberta-base/blobs/463f3414782c1c9405828c9b31bfa36dda1f45c5.lock\n",
      "2023-04-28 02:03:01 - Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Create student model from scratch\")\n",
    "word_embedding_model = models.Transformer(student_model_name, max_seq_length=max_seq_length)\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "student_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = ['parallel-sentences/TED2020-en-de-train.tsv.gz']\n",
    "dev_files = ['parallel-sentences/TED2020-en-de-dev.tsv.gz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 02:16:00 - Load parallel-sentences/TED2020-en-de-train.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "###### Read Parallel Sentences Dataset ######\n",
    "train_data = ParallelSentencesDataset(student_model=student_model, teacher_model=teacher_model, batch_size=inference_batch_size, use_embedding_cache=True)\n",
    "for train_file in train_files:\n",
    "    train_data.load_data(train_file, max_sentences=max_sentences_per_language, max_sentence_length=train_max_sentence_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.MSELoss(model=student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 02:16:05 - Create evaluator for parallel-sentences/TED2020-en-de-dev.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "#### Evaluate cross-lingual performance on different tasks #####\n",
    "evaluators = []         #evaluators has a list of different evaluator classes we call periodically\n",
    "\n",
    "for dev_file in dev_files:\n",
    "    logger.info(\"Create evaluator for \" + dev_file)\n",
    "    src_sentences = []\n",
    "    trg_sentences = []\n",
    "    with gzip.open(dev_file, 'rt', encoding='utf8') as fIn:\n",
    "        for line in fIn:\n",
    "            splits = line.strip().split('\\t')\n",
    "            if splits[0] != \"\" and splits[1] != \"\":\n",
    "                src_sentences.append(splits[0])\n",
    "                trg_sentences.append(splits[1])\n",
    "\n",
    "\n",
    "    #Mean Squared Error (MSE) measures the (euclidean) distance between teacher and student embeddings\n",
    "    dev_mse = evaluation.MSEEvaluator(src_sentences, trg_sentences, name=os.path.basename(dev_file), teacher_model=teacher_model, batch_size=inference_batch_size)\n",
    "    evaluators.append(dev_mse)\n",
    "\n",
    "    # TranslationEvaluator computes the embeddings for all parallel sentences. It then check if the embedding of source[i] is the closest to target[i] out of all available target sentences\n",
    "    dev_trans_acc = evaluation.TranslationEvaluator(src_sentences, trg_sentences, name=os.path.basename(dev_file),batch_size=inference_batch_size)\n",
    "    evaluators.append(dev_trans_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Read cross-lingual Semantic Textual Similarity (STS) data ####\n",
    "all_languages = list(set(list(source_languages)+list(target_languages)))\n",
    "sts_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/STS2017-extended.zip does not exists. Try to download from server\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffbe6c4ec1ff4615ad423382e9bc58f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/96.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This function downloads a corpus if it does not exist\n",
    "def download_corpora(filepaths):\n",
    "    if not isinstance(filepaths, list):\n",
    "        filepaths = [filepaths]\n",
    "\n",
    "    for filepath in filepaths:\n",
    "        if not os.path.exists(filepath):\n",
    "            print(filepath, \"does not exists. Try to download from server\")\n",
    "            filename = os.path.basename(filepath)\n",
    "            url = \"https://sbert.net/datasets/\" + filename\n",
    "            sentence_transformers.util.http_get(url, filepath)\n",
    "\n",
    "\n",
    "# Here we define train train and dev corpora\n",
    "train_corpus = \"datasets/ted2020.tsv.gz\"         # Transcripts of TED talks, crawled 2020\n",
    "sts_corpus = \"datasets/STS2017-extended.zip\"     # Extended STS2017 dataset for more languages\n",
    "parallel_sentences_folder = \"parallel-sentences/\"\n",
    "\n",
    "# Check if the file exists. If not, they are downloaded\n",
    "download_corpora([sts_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the ZIP File of STS2017-extended.zip and check for which language combinations we have STS data\n",
    "with zipfile.ZipFile(sts_corpus) as zip:\n",
    "    filelist = zip.namelist()\n",
    "    sts_files = []\n",
    "\n",
    "    for i in range(len(all_languages)):\n",
    "        for j in range(i, len(all_languages)):\n",
    "            lang1 = all_languages[i]\n",
    "            lang2 = all_languages[j]\n",
    "            filepath = 'STS2017-extended/STS.{}-{}.txt'.format(lang1, lang2)\n",
    "            if filepath not in filelist:\n",
    "                lang1, lang2 = lang2, lang1\n",
    "                filepath = 'STS2017-extended/STS.{}-{}.txt'.format(lang1, lang2)\n",
    "\n",
    "            if filepath in filelist:\n",
    "                filename = os.path.basename(filepath)\n",
    "                sts_data[filename] = {'sentences1': [], 'sentences2': [], 'scores': []}\n",
    "\n",
    "                fIn = zip.open(filepath)\n",
    "                for line in io.TextIOWrapper(fIn, 'utf8'):\n",
    "                    sent1, sent2, score = line.strip().split(\"\\t\")\n",
    "                    score = float(score)\n",
    "                    sts_data[filename]['sentences1'].append(sent1)\n",
    "                    sts_data[filename]['sentences2'].append(sent2)\n",
    "                    sts_data[filename]['scores'].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, data in sts_data.items():\n",
    "    test_evaluator = evaluation.EmbeddingSimilarityEvaluator(data['sentences1'], data['sentences2'], data['scores'], batch_size=inference_batch_size, name=filename, show_progress_bar=False)\n",
    "    evaluators.append(test_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 02:21:18.510130: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-28 02:21:19.542436: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd7b8c388b6472e92fcdc3ec2e6b4df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f2de1e1cef4751b7b79c7e5f0e08b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/6886 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anvisa/.local/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:547: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  labels = torch.tensor(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 02:26:14 - MSE evaluation (lower = better) on TED2020-en-de-dev.tsv.gz dataset in epoch 0 after 1000 steps:\n",
      "2023-04-28 02:26:14 - MSE (*100):\t29.767743\n",
      "2023-04-28 02:26:14 - Evaluating translation matching Accuracy on TED2020-en-de-dev.tsv.gz dataset in epoch 0 after 1000 steps:\n",
      "2023-04-28 02:27:52 - Accuracy src2trg: 4.59\n",
      "2023-04-28 02:27:52 - Accuracy trg2src: 5.20\n",
      "2023-04-28 02:27:52 - EmbeddingSimilarityEvaluator: Evaluating the model on STS.en-de.txt dataset in epoch 0 after 1000 steps:\n",
      "2023-04-28 02:27:52 - Cosine-Similarity :\tPearson: 0.0323\tSpearman: 0.0742\n",
      "2023-04-28 02:27:52 - Manhattan-Distance:\tPearson: 0.0159\tSpearman: 0.0230\n",
      "2023-04-28 02:27:52 - Euclidean-Distance:\tPearson: 0.0256\tSpearman: 0.0352\n",
      "2023-04-28 02:27:52 - Dot-Product-Similarity:\tPearson: 0.0417\tSpearman: 0.0399\n",
      "2023-04-28 02:27:52 - EmbeddingSimilarityEvaluator: Evaluating the model on STS.en-en.txt dataset in epoch 0 after 1000 steps:\n",
      "2023-04-28 02:27:53 - Cosine-Similarity :\tPearson: 0.0187\tSpearman: 0.1572\n",
      "2023-04-28 02:27:53 - Manhattan-Distance:\tPearson: 0.0858\tSpearman: 0.2573\n",
      "2023-04-28 02:27:53 - Euclidean-Distance:\tPearson: 0.0862\tSpearman: 0.1810\n",
      "2023-04-28 02:27:53 - Dot-Product-Similarity:\tPearson: 0.0381\tSpearman: -0.0033\n",
      "2023-04-28 02:27:53 - Save model to output/make-multilingual-en-de-2023-04-28_01-54-34\n",
      "2023-04-28 02:32:51 - MSE evaluation (lower = better) on TED2020-en-de-dev.tsv.gz dataset in epoch 0 after 2000 steps:\n",
      "2023-04-28 02:32:51 - MSE (*100):\t26.548558\n",
      "2023-04-28 02:32:51 - Evaluating translation matching Accuracy on TED2020-en-de-dev.tsv.gz dataset in epoch 0 after 2000 steps:\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_13467/127551991.py\", line 2, in <module>\n",
      "    student_model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py\", line 735, in fit\n",
      "    self._eval_during_training(evaluator, output_path, save_best_model, epoch, training_steps, callback)\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py\", line 777, in _eval_during_training\n",
      "    score = evaluator(self, output_path=eval_path, epoch=epoch, steps=steps)\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/sentence_transformers/evaluation/SequentialEvaluator.py\", line 18, in __call__\n",
      "    scores.append(evaluator(model, output_path, epoch, steps))\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/sentence_transformers/evaluation/TranslationEvaluator.py\", line 66, in __call__\n",
      "    cos_sims = pytorch_cos_sim(embeddings1, embeddings2).detach().cpu().numpy()\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/sentence_transformers/util.py\", line 28, in pytorch_cos_sim\n",
      "    return cos_sim(a, b)\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/sentence_transformers/util.py\", line 49, in cos_sim\n",
      "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.85 GiB (GPU 0; 23.68 GiB total capacity; 4.12 GiB already allocated; 4.58 GiB free; 17.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 953, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1005, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/stack_data/core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/usr/lib/python3/dist-packages/pygments/style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"/usr/lib/python3/dist-packages/pygments/style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "student_model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluation.SequentialEvaluator(evaluators, main_score_function=lambda scores: np.mean(scores)),\n",
    "          epochs=num_epochs,\n",
    "          warmup_steps=num_warmup_steps,\n",
    "          evaluation_steps=num_evaluation_steps,\n",
    "          output_path=output_path,\n",
    "          save_best_model=True,\n",
    "          optimizer_params= {'lr': 2e-5, 'eps': 1e-6}\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_mse = evaluation.MSEEvaluator(src_sentences, trg_sentences, teacher_model=teacher_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sentence_transformers.evaluation.MSEEvaluator.MSEEvaluator at 0x7f53c6d3d880>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = ['We need to get the shape just right.', 'So we needed to figure out how to gain control over their shape.', 'We took a radically different approach from previous efforts.']\n",
    "sentences2 = ['Die Form muss genau stimmen.', 'Also müssen wir lernen, die Form zu kontrollieren.', 'Wir machten es radikal anders, als alle vor uns.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_13467/2280941707.py\", line 1, in <module>\n",
      "    sts_evaluator = evaluation.EmbeddingSimilarityEvaluatorFromList(sentences1, sentences2, scores)\n",
      "AttributeError: module 'sentence_transformers.evaluation' has no attribute 'EmbeddingSimilarityEvaluatorFromList'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 953, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1005, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"/home/anvisa/.local/lib/python3.8/site-packages/stack_data/core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/usr/lib/python3/dist-packages/pygments/style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"/usr/lib/python3/dist-packages/pygments/style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "sts_evaluator = evaluation.EmbeddingSimilarityEvaluatorFromList(sentences1, sentences2, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
